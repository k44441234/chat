# import streamlit as st
# from streamlit_chat import message
# import numpy as np
# import faiss
# import torch
# import json
# import os
# from openai import OpenAI
# from sentence_transformers import SentenceTransformer

# st.markdown("""
#     <style>
#     .bot-response {
#         color: red;
#         font-weight: bold;
#     }
#     </style>
#     """, unsafe_allow_html=True)

# with open("data.json", encoding='utf-8') as f:
#     data = json.load(f)

# sentence = []
# for context in data:
#     sentence.append(context["content"])

# def chat_message(content, is_user=False, key=None):
#     alignment = "text-align: right;" if is_user else "text-align: right;"
#     direction = "direction: rtl;" if is_user else "direction: rtl;"
#     background_color = "#cccccc" if is_user else "#1b2331"
#     text_color = "color: #000000;"  if is_user else "color: #ffffff;"
#     st.markdown(
#         f"""
#         <div style="{direction} {alignment} {text_color} background-color: {background_color}; 
#                      border: 1px solid #ccc; padding: 10px; margin-bottom: 10px; 
#                      border-radius: 10px;">
#             {content}
#         </div>
#         """,
#         unsafe_allow_html=True
#     )

# # Initialize session state
# if 'generated' not in st.session_state:
#     st.session_state['generated'] = []
# if 'past' not in st.session_state:
#     st.session_state['past'] = []

# # ğŸ§¼ Add Clear Chat button
# if st.button("Ú¯ÙØª Ùˆ Ú¯Ùˆ Ø¬Ø¯ÛŒØ¯"):
#     st.session_state['generated'] = []
#     st.session_state['past'] = []

# def get_text():
#     col1, col2 = st.columns([5, 0.5])
#     with col1:
#         input_text = st.text_input("", key="input")
#     with col2:
#         top_k = st.number_input("ØªØ¹Ø¯Ø§Ø¯ Ø­Ø¯ÛŒØ«", min_value=1, max_value=20, value=5, step=1, label_visibility="visible")
#     return input_text, top_k


# st.markdown(
#     """
#     <h1 style='text-align: Right; direction: rtl;'>Ú†Øª Ø¨Ø§Øª Ø§Ø­Ø§Ø¯ÛŒØ«</h1>
#     """, 
#     unsafe_allow_html=True
# )

# search_text, top_k = get_text()

# model_name = "hamtaai/bg3_model".strip()
# model = SentenceTransformer(model_name, device="cpu")

# embedding_file = "embeddings.npy"

# if os.path.exists(embedding_file):
#     print("Loading existing embeddings from file...")
#     embeddings = np.load(embedding_file)
# else:
#     print("Embeddings file not found. Computing embeddings...")
#     embeddings = model.encode(sentence)
#     np.save(embedding_file, embeddings)
#     print("Embeddings saved to file.")

# vector_dimension = embeddings.shape[1]
# index = faiss.IndexFlatL2(vector_dimension)
# faiss.normalize_L2(embeddings)
# index.add(embeddings)
# print("Embeddings added to the FAISS index.")

# def final_text(query, top_k):
#     search_vector = model.encode(query)
#     _vector = np.array([search_vector])
#     faiss.normalize_L2(_vector)
#     distances, ann = index.search(_vector, top_k)
#     final_text = []
#     for item in ann[0]:
#         final_text.append(sentence[item])
#     return final_text

# client = OpenAI(
#     api_key=st.secrets["openai_key"],
# )

# def generate_answer(context, query):
#     prompt = f"Ø§Ø­Ø§Ø¯ÛŒØ«: {context}\nØ³Ø¤Ø§Ù„: {query}"
#     response = client.chat.completions.create(
#         model="gpt-4o-mini",
#         messages=[
#             {
#                 "role": "system",
#                 "content": """Ø´Ù…Ø§ ÛŒÚ© Ú†Øª Ø¨Ø§Øª Ù…Ø®ØªØµØµ Ø§Ø­Ø§Ø¯ÛŒØ« Ù‡Ø³Øª
#                 Ù„Ø·ÙØ§ ØªÙ†Ù‡Ø§ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø­Ø§Ø¯ÛŒØ« Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡"""
#             },
#             {
#                 "role": "user",
#                 "content": prompt
#             }
#         ]
#     )
#     return response.choices[0].message.content if response.choices else "No response received."

# def rag(query, top_k=5):
#     retrieved_docs = final_text(query, top_k)
#     combined_context = "\n Ø­Ø¯ÛŒØ«: \n\n".join(retrieved_docs)
#     answer = generate_answer(combined_context, query)
#     return answer , query, combined_context  

# if search_text:
#     with st.spinner("Generating response..."):  
#         bot_response, context, combined_context  = rag(search_text, top_k)

#         response = bot_response
#         st.session_state.past.append(context)
#         st.session_state.generated.append(response)
#         with st.expander("ğŸ“„ View retrieved context used for this answer"):
#             st.write(combined_context)

# if st.session_state['generated']:
#     for i in range(len(st.session_state['generated'])):
#         chat_message(st.session_state["generated"][i], is_user=False, key=str(i))









import streamlit as st
from streamlit_chat import message
import numpy as np
import faiss
import torch
import json
import os
from openai import OpenAI
from sentence_transformers import SentenceTransformer

st.markdown("""
    <style>
        body, .stApp {
            direction: rtl;
            text-align: right;
            font-family: "Tahoma", "Vazirmatn", sans-serif;
        }
        .bot-response {
            color: red;
            font-weight: bold;
        }
        .stTextInput > div > div > input {
            text-align: right !important;
        }
        .stNumberInput input {
            text-align: center;
        }
        h1, h2, h3, h4 {
            text-align: right !important;
        }
    </style>
    """, unsafe_allow_html=True)

with open("data.json", encoding='utf-8') as f:
    data = json.load(f)

sentence = [context["content"] for context in data]

def chat_message(content, is_user=False, key=None):
    background_color = "#cccccc" if is_user else "#1b2331"
    text_color = "#000000" if is_user else "#ffffff"
    st.markdown(
        f"""
        <div style="direction: rtl; text-align: right; color: {text_color}; 
                    background-color: {background_color}; 
                    border: 1px solid #ccc; padding: 10px; margin-bottom: 10px; 
                    border-radius: 10px;">
            {content}
        </div>
        """,
        unsafe_allow_html=True
    )


if 'generated' not in st.session_state:
    st.session_state['generated'] = []
if 'past' not in st.session_state:
    st.session_state['past'] = []


# Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú†Øª Ùˆ ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ø¯Ú©Ù…Ù‡
if st.button("ğŸ” Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆÛŒ Ø¬Ø¯ÛŒØ¯"):
    st.session_state['generated'] = []
    st.session_state['past'] = []
    st.session_state['input'] = ""  # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ
    st.experimental_rerun()         # Ø±ÛŒÙØ±Ø´ ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ



def get_text():
    col1, col2 = st.columns([5, 1])
    with col1:
        input_text = st.text_input("Ù¾ÛŒØ§Ù… Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:", key="input")
    with col2:
        top_k = st.number_input("ØªØ¹Ø¯Ø§Ø¯ Ø§Ø­Ø§Ø¯ÛŒØ«", min_value=1, max_value=20, value=5, step=1, label_visibility="visible")
    return input_text, top_k


st.markdown(
    """
    <h1 style='text-align: right;'> Ú†Øªâ€ŒØ¨Ø§Øª Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒ Ø§Ø­Ø§Ø¯ÛŒØ«</h1>
    """,
    unsafe_allow_html=True
)

search_text, top_k = get_text()


model_name = "hamtaai/bg3_model".strip()
model = SentenceTransformer(model_name, device="cpu")

embedding_file = "embeddings.npy"
if os.path.exists(embedding_file):
    embeddings = np.load(embedding_file)
else:
    embeddings = model.encode(sentence)
    np.save(embedding_file, embeddings)

vector_dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(vector_dimension)
faiss.normalize_L2(embeddings)
index.add(embeddings)

def final_text(query, top_k):
    search_vector = model.encode(query)
    _vector = np.array([search_vector])
    faiss.normalize_L2(_vector)
    distances, ann = index.search(_vector, top_k)
    return [sentence[item] for item in ann[0]]

client = OpenAI(
    api_key=st.secrets["openai_key"],
)

def generate_answer(context, query):
    prompt = f"Ø§Ø­Ø§Ø¯ÛŒØ«: {context}\nØ³Ø¤Ø§Ù„: {query}"
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": """Ø´Ù…Ø§ ÛŒÚ© Ú†Øªâ€ŒØ¨Ø§Øª Ù…ØªØ®ØµØµ Ø§Ø­Ø§Ø¯ÛŒØ« Ù‡Ø³ØªÛŒØ¯.
                Ù„Ø·ÙØ§Ù‹ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ø­Ø§Ø¯ÛŒØ« Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯."""
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
    )
    return response.choices[0].message.content if response.choices else "Ù¾Ø§Ø³Ø®ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯."

def rag(query, top_k=5):
    retrieved_docs = final_text(query, top_k)
    combined_context = "\nØ­Ø¯ÛŒØ«:\n\n".join(retrieved_docs)
    answer = generate_answer(combined_context, query)
    return answer, query, combined_context

if search_text:
    with st.spinner("â³ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®..."):
        bot_response, context, combined_context = rag(search_text, top_k)
        st.session_state.past.append(context)
        st.session_state.generated.append(bot_response)
        with st.expander("ğŸ“œ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø§Ø­Ø§Ø¯ÛŒØ« Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡"):
            st.write(combined_context)

if st.session_state['generated']:
    for i in range(len(st.session_state['generated'])):
        chat_message(st.session_state["generated"][i], is_user=False, key=str(i))
